{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j46WeVhFvay",
        "outputId": "3910410f-7247-4a47-f62a-ea49b6dd88fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.autograd import Variable\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_gray= 0.1307\n",
        "stddev_gray= 0.3081\n",
        "\n",
        "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((mean_gray,),(stddev_gray,))])\n",
        "\n",
        "train_dataset=datasets.MNIST(root='./data', train=True, transform=transforms, download=True)\n",
        "test_dataset=datasets.MNIST(root='./data', train=False, transform=transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN2wB9iVFx84",
        "outputId": "10694c1a-4979-45de-d01a-c2490f2dede5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 157555370.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 28309346.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 49313629.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4950761.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=100\n",
        "epochs=10\n",
        "train_load=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_load=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "KG1LIHDXGPLP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    # input_size:28, same_padding=(filter_size-1)/2, 3-1/2=1:padding\n",
        "    self.cnn1=nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "    # input_size-filter_size +2(padding)/stride + 1 = 28-3+2(1)/1+1=28\n",
        "    self.batchnorm1=nn.BatchNorm2d(8)\n",
        "    # output_channel:8, batch(8)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
        "    #input_size=28/2=14\n",
        "    self.cnn2=nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "    # same_padding: (5-1)/2=2:padding_size.\n",
        "    self.batchnorm2=nn.BatchNorm2d(32)\n",
        "    self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
        "    # input_size=14/2=7\n",
        "    # 32x7x7=1568\n",
        "    self.fc1 =nn.Linear(in_features=1568, out_features=600)\n",
        "    self.dropout= nn.Dropout(p=0.5)\n",
        "    self.fc2 =nn.Linear(in_features=600, out_features=10)\n",
        "  def forward(self,x):\n",
        "    out =self.cnn1(x)\n",
        "    out =self.batchnorm1(out)\n",
        "    out =self.relu(out)\n",
        "    out =self.maxpool1(out)\n",
        "    out =self.cnn2(out)\n",
        "    out =self.batchnorm2(out)\n",
        "    out =self.relu(out)\n",
        "    out =self.maxpool2(out)\n",
        "    out =out.view(-1,1568)\n",
        "    out =self.fc1(out)\n",
        "    out =self.relu(out)\n",
        "    out =self.dropout(out)\n",
        "    out =self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "_sWnz__pGRuQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=CNN()\n",
        "CUDA=torch.cuda.is_available()\n",
        "if CUDA:\n",
        "  model=model.cuda()\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adadelta(model.parameters(), lr=0.01)\n",
        ""
      ],
      "metadata": {
        "id": "RCCEgx4lGXLu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iteration=0\n",
        "correct_nodata=0\n",
        "correct_data=0\n",
        "for i,(inputs,labels) in enumerate (train_load):\n",
        "  if iteration==1:\n",
        "    break\n",
        "  inputs=Variable(inputs)\n",
        "  labels=Variable(labels)\n",
        "  if torch.cuda.is_available():\n",
        "    inputs=inputs.cuda()\n",
        "    labels=labels.cuda()\n",
        "  print(\"For 1 iteration, this is what happens:\")\n",
        "  print(\"Input Shape:\",inputs.shape)\n",
        "  print(\"Labels Shape:\", labels.shape)\n",
        "  output = model(inputs)\n",
        "  print(\"Output Shape:\",output.shape)\n",
        "  _,predicted_nodata=torch.max(output,1)\n",
        "  print(\"Predicted Shape:\",predicted_nodata.shape)\n",
        "  print(\"Predicted Tensor:\",predicted_nodata)\n",
        "  correct_nodata +=(predicted_nodata==labels).sum()\n",
        "  print(\"Correct Predictions:\",correct_nodata)\n",
        "  _,predicted_data = torch.max(output.data,1)\n",
        "  correct_data +=(predicted_data==labels.data).sum()\n",
        "  print(\"Correct Predictions:\",correct_data)\n",
        "\n",
        "  iteration+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qznqn5MNGYOs",
        "outputId": "6003ca0a-ba4c-49c0-9c29-dfa3fe96a91a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For 1 iteration, this is what happens:\n",
            "Input Shape: torch.Size([100, 1, 28, 28])\n",
            "Labels Shape: torch.Size([100])\n",
            "Output Shape: torch.Size([100, 10])\n",
            "Predicted Shape: torch.Size([100])\n",
            "Predicted Tensor: tensor([7, 5, 0, 2, 7, 5, 2, 9, 6, 7, 5, 3, 3, 7, 6, 5, 5, 5, 1, 2, 0, 7, 1, 7,\n",
            "        1, 3, 2, 1, 3, 1, 2, 6, 1, 5, 1, 7, 5, 1, 5, 7, 2, 2, 1, 2, 0, 1, 2, 3,\n",
            "        9, 5, 3, 5, 3, 5, 3, 2, 2, 2, 7, 1, 0, 3, 3, 7, 2, 3, 9, 7, 2, 0, 3, 1,\n",
            "        5, 4, 3, 5, 5, 3, 5, 7, 7, 2, 3, 3, 2, 3, 9, 2, 7, 4, 2, 3, 7, 5, 5, 7,\n",
            "        2, 3, 6, 2], device='cuda:0')\n",
            "Correct Predictions: tensor(13, device='cuda:0')\n",
            "Correct Predictions: tensor(13, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=25\n",
        "\n",
        "train_loss=[]\n",
        "test_loss=[]\n",
        "train_accuracy=[]\n",
        "test_accuracy=[]\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "  # Reset variables at 0 epoch\n",
        "  correct=0\n",
        "  iteration=0\n",
        "  iter_loss=0.0\n",
        "\n",
        "  model.train() # Training Mode\n",
        "\n",
        "  for i,(inputs,labels) in enumerate(train_load):\n",
        "\n",
        "    inputs=Variable(inputs)\n",
        "    labels=Variable(labels)\n",
        "\n",
        "    # if CUDA is avaible, shift to GPU (CUDA)\n",
        "    CUDA=torch.cuda.is_available()\n",
        "    if CUDA:\n",
        "      inputs=inputs.cuda()\n",
        "      labels=labels.cuda()\n",
        "\n",
        "    optimizer.zero_grad() # clear gradient\n",
        "    outputs=model(inputs)\n",
        "    loss=loss_fn(outputs,labels)\n",
        "    iter_loss += loss.item() # Accumulate loss\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step() # update weights\n",
        "\n",
        "    # Save the correct predictions for training data\n",
        "    _,predicted=torch.max(outputs,1)\n",
        "    correct +=(predicted==labels).sum()\n",
        "    iteration +=1\n",
        "\n",
        "  train_loss.append(iter_loss/iteration)\n",
        "  train_accuracy.append((100*correct/len(train_dataset)))\n",
        "\n",
        "  # Testing\n",
        "  correct=0\n",
        "  iteration=0\n",
        "  loss=0.0\n",
        "\n",
        "  model.eval()  # Testing Mode\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(test_load):\n",
        "\n",
        "    inputs=Variable(inputs)\n",
        "    labels=Variable(labels)\n",
        "\n",
        "    CUDA=torch.cuda.is_available()\n",
        "    if CUDA:\n",
        "      inputs=inputs.cuda()\n",
        "      labels=labels.cuda()\n",
        "\n",
        "    outputs=model(inputs)\n",
        "    loss=loss_fn(outputs,labels)\n",
        "    loss += loss.item()\n",
        "\n",
        "    _,predicted=torch.max(outputs,1)\n",
        "    correct+=(predicted==labels).sum()\n",
        "\n",
        "    iteration+=1\n",
        "\n",
        "  test_loss.append(loss/iteration)\n",
        "  test_accuracy.append((100*correct/len(test_dataset)))\n",
        "\n",
        "  print('Epoch {}/{}, Training Loss:{:.3f}, Training Accuracy:{:.3f}, Testing Loss {:.3f}, Testing Accuracy:{:.3f}'\n",
        "       .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1]))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9orprmY9GZtj",
        "outputId": "f9d8f196-eb42-4958-f573-f2e4b2dd69d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Training Loss:0.859, Training Accuracy:78.400, Testing Loss 0.008, Testing Accuracy:92.570\n",
            "Epoch 2/25, Training Loss:0.303, Training Accuracy:91.837, Testing Loss 0.006, Testing Accuracy:94.970\n",
            "Epoch 3/25, Training Loss:0.209, Training Accuracy:94.222, Testing Loss 0.005, Testing Accuracy:96.250\n",
            "Epoch 4/25, Training Loss:0.166, Training Accuracy:95.267, Testing Loss 0.004, Testing Accuracy:96.880\n",
            "Epoch 5/25, Training Loss:0.139, Training Accuracy:96.095, Testing Loss 0.003, Testing Accuracy:97.390\n",
            "Epoch 6/25, Training Loss:0.121, Training Accuracy:96.505, Testing Loss 0.003, Testing Accuracy:97.650\n",
            "Epoch 7/25, Training Loss:0.109, Training Accuracy:96.910, Testing Loss 0.002, Testing Accuracy:97.830\n",
            "Epoch 8/25, Training Loss:0.099, Training Accuracy:97.108, Testing Loss 0.002, Testing Accuracy:97.950\n",
            "Epoch 9/25, Training Loss:0.092, Training Accuracy:97.318, Testing Loss 0.002, Testing Accuracy:98.010\n",
            "Epoch 10/25, Training Loss:0.086, Training Accuracy:97.460, Testing Loss 0.002, Testing Accuracy:98.090\n",
            "Epoch 11/25, Training Loss:0.080, Training Accuracy:97.647, Testing Loss 0.002, Testing Accuracy:98.300\n",
            "Epoch 12/25, Training Loss:0.074, Training Accuracy:97.797, Testing Loss 0.001, Testing Accuracy:98.300\n",
            "Epoch 13/25, Training Loss:0.071, Training Accuracy:97.952, Testing Loss 0.001, Testing Accuracy:98.330\n",
            "Epoch 14/25, Training Loss:0.067, Training Accuracy:98.073, Testing Loss 0.001, Testing Accuracy:98.470\n",
            "Epoch 15/25, Training Loss:0.066, Training Accuracy:98.023, Testing Loss 0.001, Testing Accuracy:98.470\n",
            "Epoch 16/25, Training Loss:0.062, Training Accuracy:98.163, Testing Loss 0.001, Testing Accuracy:98.500\n",
            "Epoch 17/25, Training Loss:0.059, Training Accuracy:98.263, Testing Loss 0.001, Testing Accuracy:98.570\n",
            "Epoch 18/25, Training Loss:0.058, Training Accuracy:98.303, Testing Loss 0.001, Testing Accuracy:98.600\n",
            "Epoch 19/25, Training Loss:0.056, Training Accuracy:98.352, Testing Loss 0.001, Testing Accuracy:98.660\n",
            "Epoch 20/25, Training Loss:0.054, Training Accuracy:98.450, Testing Loss 0.001, Testing Accuracy:98.680\n",
            "Epoch 21/25, Training Loss:0.052, Training Accuracy:98.482, Testing Loss 0.001, Testing Accuracy:98.700\n",
            "Epoch 22/25, Training Loss:0.051, Training Accuracy:98.450, Testing Loss 0.001, Testing Accuracy:98.750\n",
            "Epoch 23/25, Training Loss:0.049, Training Accuracy:98.573, Testing Loss 0.001, Testing Accuracy:98.780\n",
            "Epoch 24/25, Training Loss:0.048, Training Accuracy:98.582, Testing Loss 0.001, Testing Accuracy:98.780\n",
            "Epoch 25/25, Training Loss:0.046, Training Accuracy:98.645, Testing Loss 0.001, Testing Accuracy:98.780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img= test_dataset[40][0].resize_((1,1,28,28))\n",
        "img= Variable(img)\n",
        "label= test_dataset[40][1]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model=model.cuda()\n",
        "  img=img.cuda()\n",
        "\n",
        "output=model(img)\n",
        "print(output)\n",
        "print(output.data)\n",
        "_,predicted=torch.max(output,1)\n",
        "print(\"Prediction is:\",predicted.item())\n",
        "print(\"Actual is:\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDUe8HHLGeMO",
        "outputId": "80952f1f-436e-4cc2-e349-fe9a5bc693fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.4590,  5.4726, -3.0695, -3.8208, -1.1391, -3.9414, -5.2704, -1.1731,\n",
            "         -2.7312, -2.5211]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.4590,  5.4726, -3.0695, -3.8208, -1.1391, -3.9414, -5.2704, -1.1731,\n",
            "         -2.7312, -2.5211]], device='cuda:0')\n",
            "Prediction is: 1\n",
            "Actual is: 1\n"
          ]
        }
      ]
    }
  ]
}